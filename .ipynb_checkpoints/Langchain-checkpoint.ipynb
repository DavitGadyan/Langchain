{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2b9f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d806e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f533e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ -----------\n",
      "aiofiles                 23.2.1\n",
      "aiohttp                  3.9.3\n",
      "aiosignal                1.3.1\n",
      "annotated-types          0.6.0\n",
      "anyio                    4.3.0\n",
      "appnope                  0.1.4\n",
      "appopener                1.6\n",
      "asttokens                2.4.1\n",
      "attrs                    23.2.0\n",
      "certifi                  2024.2.2\n",
      "charset-normalizer       3.3.2\n",
      "comm                     0.2.2\n",
      "dataclasses-json         0.6.4\n",
      "debugpy                  1.8.1\n",
      "decorator                5.1.1\n",
      "deepgram-sdk             3.2.5\n",
      "executing                2.0.1\n",
      "frozenlist               1.4.1\n",
      "greenlet                 3.0.3\n",
      "h11                      0.14.0\n",
      "httpcore                 1.0.5\n",
      "httpx                    0.27.0\n",
      "idna                     3.6\n",
      "ipykernel                6.29.4\n",
      "ipython                  8.23.0\n",
      "jedi                     0.19.1\n",
      "jsonpatch                1.33\n",
      "jsonpointer              2.4\n",
      "jupyter_client           8.6.1\n",
      "jupyter_core             5.7.2\n",
      "langchain                0.1.16\n",
      "langchain-community      0.0.32\n",
      "langchain-core           0.1.42\n",
      "langchain-text-splitters 0.0.1\n",
      "langsmith                0.1.47\n",
      "marshmallow              3.21.1\n",
      "matplotlib-inline        0.1.6\n",
      "multidict                6.0.5\n",
      "mypy-extensions          1.0.0\n",
      "nest-asyncio             1.6.0\n",
      "numpy                    1.26.4\n",
      "orjson                   3.10.0\n",
      "packaging                23.2\n",
      "parso                    0.8.4\n",
      "pexpect                  4.9.0\n",
      "pip                      24.0\n",
      "platformdirs             4.2.0\n",
      "prompt-toolkit           3.0.43\n",
      "psutil                   5.9.8\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "PyAudio                  0.2.14\n",
      "pydantic                 2.7.0\n",
      "pydantic_core            2.18.1\n",
      "Pygments                 2.17.2\n",
      "python-dateutil          2.9.0.post0\n",
      "python-dotenv            1.0.1\n",
      "PyYAML                   6.0.1\n",
      "pyzmq                    25.1.2\n",
      "requests                 2.31.0\n",
      "six                      1.16.0\n",
      "sniffio                  1.3.1\n",
      "SQLAlchemy               2.0.29\n",
      "stack-data               0.6.3\n",
      "tenacity                 8.2.3\n",
      "tornado                  6.4\n",
      "traitlets                5.14.2\n",
      "typing_extensions        4.11.0\n",
      "typing-inspect           0.9.0\n",
      "urllib3                  2.2.1\n",
      "verboselogs              1.7\n",
      "wcwidth                  0.2.13\n",
      "websockets               12.0\n",
      "wheel                    0.43.0\n",
      "yarl                     1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bb8d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71f04a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('creds.yaml') as f:\n",
    "    creds = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0af621db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whatever you do, take care of your shoes.\n",
      "I'm a big fan of the \"take care of your shoes\" philosophy. I've had a lot of shoes in my life, and I've learned that if you take care of them, they will take care of you.\n",
      "I've had a lot of shoes in my life, and I've learned that if you take care of them, they will take care of you.\n",
      "I've had a lot of shoes in my life, and I\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# Set your Hugging Face API token \n",
    "huggingfacehub_api_token = creds[\"HUGGING_FACE_TOKEN\"]\n",
    "\n",
    "# Define the LLM\n",
    "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9d73b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c171c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(53486) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". How your shoes look and feel can greatly impact your overall appearance and confidence. Here are some tips for taking care of your shoes:\n",
      "\n",
      "1. Clean and polish regularly: Regularly clean your shoes with a damp cloth to remove any dirt or debris. Use a shoe polish that matches the color of your shoes to keep them looking shiny and new.\n",
      "\n",
      "2. Rotate your shoes: Wearing the same pair of shoes every day can cause them to wear out quickly. Rotate your shoes and give them a break to prolong their lifespan.\n",
      "\n",
      "3. Invest in shoe trees: Shoe trees help maintain the shape of your shoes and prevent creasing. They also absorb moisture and odor, keeping your shoes fresh.\n",
      "\n",
      "4. Protect from the elements: Use a waterproof spray to protect your shoes from rain and snow. This will also help prevent stains and damage to the material.\n",
      "\n",
      "5. Store them properly: When not in use, store your shoes in a cool, dry place. Avoid leaving them in direct sunlight or in a damp environment, as this can cause them to warp or develop mold.\n",
      "\n",
      "6. Fix any damage immediately: If you notice any scuffs or scratches on your shoes, address them right away. Use a leather conditioner or polish to cover up any marks and prevent them from getting worse.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "# Define the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62eb10ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an artificial intelligence assistant, answer the question. How does LangChain make LLM application development easier?\n",
      "LangChain provides a platform for LLM application development that simplifies the process by offering a range of tools and services. It allows developers to focus on the core of their application while handling the low-level details such as database management, authentication, and authorization. LangChain also offers a variety of pre-built modules and components that can be easily integrated into LLM applications, reducing development time and cost. Additionally, LangChain provides a secure and scalable infrastructure that can handle large volumes of traffic and data\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Set your Hugging Face API token\n",
    "huggingfacehub_api_token = creds[\"HUGGING_FACE_TOKEN\"]\n",
    "\n",
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c0a953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To retain learning effectively, you can try the following strategies:\\n\\n1. Review and practice regularly: Consistent review and practice of the material you have learned can help reinforce your memory.\\n\\n2. Teach others: Explaining concepts to someone else can help solidify your understanding and retention of the information.\\n\\n3. Use mnemonic devices: Mnemonic devices such as acronyms, rhymes, or visual imagery can help you remember key information.\\n\\n4. Stay engaged: Stay actively engaged in the learning process by asking questions, participating in discussions, and seeking out additional resources.\\n\\n5. Apply what you have learned: Applying the knowledge you have gained in real-life situations can help reinforce your understanding and retention.\\n\\n6. Take breaks: Give yourself regular breaks during study sessions to prevent burnout and improve information retention.\\n\\nBy incorporating these strategies into your learning routine, you can enhance your ability to retain new information effectively.', response_metadata={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 27, 'total_tokens': 207}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-83456cc5-db89-4501-996d-d7ba0b76d138-0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key= creds[\"OPEN_AI_API\"] \n",
    "\n",
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Insert a question into the template and call the model\n",
    "full_prompt = prompt_template.format_messages(question='How can I retain learning?')\n",
    "llm(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6702784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"A list comprehension is a concise way to create lists in Python. It allows you to create a new list by applying an expression to each item in an existing iterable (such as a list, tuple, or range) and filtering the items based on a condition.\\n\\nHere's a simple example of a list comprehension that creates a list of squared numbers from 0 to 9:\\n\\n```python\\nsquared_numbers = [x**2 for x in range(10)]\\nprint(squared_numbers)\\n```\\n\\nThis will output:\\n\\n```\\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\\n```\\n\\nList comprehensions are a powerful and concise way to work with lists in Python and are often preferred over traditional for loops for their readability and simplicity.\" response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 26, 'total_tokens': 193}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-d9f80501-91db-4674-81c5-4146c72d7168-0'\n",
      "content='List comprehension is a concise way to create lists in Python by applying an expression to each item in an iterable.' response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 36, 'total_tokens': 58}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-05d50bb7-1448-4e41-9266-e27ae0143750-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key=  creds[\"OPEN_AI_API\"] \n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the conversation history and add the first AI message\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hello! Ask me anything about Python programming!\")\n",
    "\n",
    "# Add the user message to the history and call the model\n",
    "history.add_user_message(\"What is a list comprehension?\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)\n",
    "\n",
    "# Add another user message and call the model\n",
    "history.add_user_message(\"Describe the same in fewer words\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e01e419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Write Python code to draw a scatter plot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Write Python code to draw a scatter plot.\n",
      "AI:  Sure, I can definitely help you with that! To draw a scatter plot in Python, you will need to import the matplotlib library. This can be done by using the command \"import matplotlib.pyplot as plt\". Then, you can create a scatter plot by using the \"scatter()\" function and passing in the x and y values as parameters. For example, if you have two lists of data called \"x_data\" and \"y_data\", you can create a scatter plot by using the code \"plt.scatter(x_data, y_data)\". You can also customize your scatter plot by adding a title, labels for the x and y axes, and changing the color and size of the data points. Is there anything else you would like to know about drawing a scatter plot in Python?\n",
      "Human: Use the Seaborn library.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ah, great question! Seaborn is another popular library for data visualization in Python. To use Seaborn to draw a scatter plot, you will need to import it using the command \"import seaborn as sns\". Then, you can use the \"scatterplot()\" function and pass in the x and y values as well as the name of the data frame containing your data. For example, if your data frame is called \"df\", you can create a scatter plot by using the code \"sns.scatterplot(x=\\'x_data\\', y=\\'y_data\\', data=df)\". Seaborn also offers many customization options for your scatter plot, such as adding a trendline or changing the shape and size of the data points. Is there anything else you would like to know about using Seaborn for scatter plots?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key =creds[\"OPEN_AI_API\"] \n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a buffer memory\n",
    "memory = ConversationBufferMemory(size=4)\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "buffer_chain.predict(input=\"Write Python code to draw a scatter plot.\")\n",
    "buffer_chain.predict(input=\"Use the Seaborn library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f81e516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Describe the relationship of the human mind with the keyboard when taking a great online class.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human and AI discuss the relationship between the human mind and keyboard in taking a great online class. The AI describes it as a collaborative and communicative partnership, with the keyboard serving as a tool for inputting information and interacting with the class. The keyboard acts as a bridge, allowing for a seamless flow of information and expression of thoughts and ideas in a written format. This enhances the overall learning experience in an online class.\n",
      "Human: Use an analogy to describe it.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hmm, let me think. I would say the relationship between the human mind and keyboard in taking a great online class is like a dance between two partners. The mind leads with its thoughts and ideas, while the keyboard follows and translates them into written form. Together, they create a beautiful and harmonious performance, enhancing the learning experience for the human. Does that make sense?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a summary memory that uses an OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key))\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "summary_chain.predict(input=\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
    "summary_chain.predict(input=\"Use an analogy to describe it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1810a",
   "metadata": {},
   "source": [
    "# RAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431ae5b",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d931e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(56493) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bbafbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.' metadata={'source': 'resources/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a document loader for attention_is_all_you_need.pdf\n",
    "loader = PyPDFLoader('resources/NIPS-2017-attention-is-all-you-need-Paper.pdf')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69866c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='country: United States\\nconfederation: CONCACAF\\npopulation_share: 4.5\\ntv_audience_share: 4.3\\ngdp_weighted_share: 11.3' metadata={'source': 'resources/fifa_countries_audience.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Create a document loader for fifa_countries_audience.csv\n",
    "loader = CSVLoader(file_path='resources/fifa_countries_audience.csv')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a8a7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(56522) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35f22fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Old CSS, new CSS (2020) (eev.ee)' metadata={'source': 'https://news.ycombinator.com', 'title': 'Old CSS, new CSS (2020) (eev.ee)', 'link': 'https://eev.ee/blog/2020/02/01/old-css-new-css/', 'ranking': '1.'}\n",
      "{'source': 'https://news.ycombinator.com', 'title': 'Old CSS, new CSS (2020) (eev.ee)', 'link': 'https://eev.ee/blog/2020/02/01/old-css-new-css/', 'ranking': '1.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import HNLoader\n",
    "\n",
    "# Create a document loader for the top Hacker News stories\n",
    "loader = HNLoader(\"https://news.ycombinator.com\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "\n",
    "# Print the first document\n",
    "print(data[0])\n",
    "\n",
    "# Print the first document's metadata\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbfc2115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.']\n"
     ]
    }
   ],
   "source": [
    "# Import libary\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "quote = 'One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote) \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "630e5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Words are flowing out', 'out like endless rain', 'rain into a paper cup,', 'they slither while they', 'they pass,', 'they slip away across', 'across the universe.']\n"
     ]
    }
   ],
   "source": [
    "# Import libary\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=chunk_size,\n",
    "  chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote) \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "715a10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a57ba64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(57673) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.11.8-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Downloading lxml-5.2.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting tabulate (from unstructured)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: requests in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured) (4.12.3)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: dataclasses-json in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured) (0.6.4)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2024.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.8.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured) (4.11.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.22.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from dataclasses-json->unstructured) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Collecting click (from nltk->unstructured)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->unstructured)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from nltk->unstructured) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from nltk->unstructured) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests->unstructured) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests->unstructured) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests->unstructured) (2024.2.2)\n",
      "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
      "  Downloading deepdiff-7.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: packaging>=23.1 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured-client->unstructured) (23.2)\n",
      "Requirement already satisfied: pypdf>=4.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured-client->unstructured) (4.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading unstructured-0.11.8-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading lxml-5.2.1-cp312-cp312-macosx_10_9_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading rapidfuzz-3.8.1-cp312-cp312-macosx_10_9_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl (37 kB)\n",
      "Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=e37d10346fbb3814659ac5677a19afd06fcc117f6beee469c026cc08c814be29\n",
      "  Stored in directory: /Users/dyada/Library/Caches/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: filetype, wrapt, tabulate, rapidfuzz, python-magic, python-iso639, ordered-set, lxml, langdetect, jsonpath-python, joblib, emoji, click, chardet, backoff, nltk, deepdiff, unstructured-client, unstructured\n",
      "Successfully installed backoff-2.2.1 chardet-5.2.0 click-8.1.7 deepdiff-7.0.1 emoji-2.11.0 filetype-1.2.0 joblib-1.4.0 jsonpath-python-1.0.6 langdetect-1.0.9 lxml-5.2.1 nltk-3.8.1 ordered-set-4.1.0 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.8.1 tabulate-0.9.0 unstructured-0.11.8 unstructured-client-0.22.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4dbaf559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(57691) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='To search this site, enter a search term\\n\\nSearch\\n\\nFACT SHEET: President\\xa0Biden Issues Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence\\n\\nHome\\n\\nBriefing Room\\n\\nStatements and Releases\\n\\nToday, President Biden is issuing a landmark Executive Order to ensure that America leads the way in seizing the promise and managing the risks of artificial intelligence (AI)', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The Executive Order establishes new standards for AI safety and security, protects Americans’ privacy, advances equity and civil rights, stands up for consumers and workers, promotes innovation and competition, advances American leadership around the world, and more', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nAs part of the Biden-Harris Administration’s comprehensive strategy for responsible innovation, the Executive Order builds on previous actions the President has taken, including work that led to voluntary commitments from 15 leading companies to drive safe, secure, and trustworthy development of AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nThe Executive Order directs the following actions:\\n\\nNew Standards for AI Safety and Security\\n\\nRequire that developers of the most powerful AI systems share their safety test results and other critical information with the U.S. government', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0In accordance with the Defense Production Act, the Order will require that companies developing any foundation model that poses a serious risk to national security, national economic security, or national public health and safety must notify the federal government when training the model, and must share the results of all red-team safety tests', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. These measures will ensure AI systems are safe, secure, and trustworthy before companies make them public.\\n\\nDevelop standards, tools, and tests to help ensure that AI systems are safe, secure, and trustworthy', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0The National Institute of Standards and Technology will set the rigorous standards for extensive red-team testing to ensure safety before public release. The Department of Homeland Security will apply those standards to critical infrastructure sectors and establish the AI Safety and Security Board', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The Departments of Energy and Homeland Security will also address AI systems’ threats to critical infrastructure, as well as chemical, biological, radiological, nuclear, and cybersecurity risks', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. Together, these are the most significant actions ever taken by any government to advance the field of AI safety.\\n\\nProtect against the risks of using AI to engineer dangerous biological materials\\xa0by developing strong new standards for biological synthesis screening', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. Agencies that fund life-science projects will establish these standards as a condition of federal funding, creating powerful incentives to ensure appropriate screening and manage risks potentially made worse by AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nProtect Americans from AI-enabled fraud and deception by establishing standards and best practices for detecting AI-generated content and authenticating official content', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The Department of Commerce will develop guidance for content authentication and watermarking to clearly label AI-generated content', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. Federal agencies will use these tools to make it easy for Americans to know that the communications they receive from their government are authentic—and set an example for the private sector and governments around the world', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nEstablish an advanced cybersecurity program to develop AI tools to find and fix vulnerabilities in critical software,\\xa0building on the Biden-Harris Administration’s ongoing AI Cyber Challenge', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. Together, these efforts will harness AI’s potentially game-changing cyber capabilities to make software and networks more secure', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nOrder the development of a National Security Memorandum that directs further actions on AI and security,\\xa0to be developed by the National Security Council and White House Chief of Staff', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. This document will ensure that the United States military and intelligence community use AI safely, ethically, and effectively in their missions, and will direct actions to counter adversaries’ military use of AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nProtecting Americans’ Privacy\\n\\nWithout safeguards, AI can put Americans’ privacy further at risk. AI not only makes it easier to extract, identify, and exploit personal data, but it also heightens incentives to do so because companies use data to train AI systems', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0To better protect Americans’ privacy, including from the risks posed by AI, the President calls on Congress to pass bipartisan data privacy legislation to protect all Americans, especially kids, and directs the following actions:\\n\\nProtect Americans’ privacy by prioritizing federal support for accelerating the development and use of privacy-preserving techniques—including ones that use cutting-edge AI and that let AI systems be trained while preserving the privacy of the training data', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nStrengthen privacy-preserving research\\xa0and technologies,\\xa0such as cryptographic tools that preserve individuals’ privacy, by funding a Research Coordination Network to advance rapid breakthroughs and development', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The National Science Foundation will also work with this network to promote the adoption of leading-edge privacy-preserving technologies by federal agencies', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nEvaluate how agencies collect and use commercially available information—including information they procure from data brokers—and\\xa0strengthen privacy guidance for federal agencies\\xa0to account for AI risks', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. This work will focus in particular on commercially available information containing personally identifiable data.\\n\\nDevelop guidelines for federal agencies to evaluate the effectiveness of privacy-preserving techniques,\\xa0including those used in AI systems', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. These guidelines will advance agency efforts to protect Americans’ data.\\n\\nAdvancing Equity and Civil Rights\\n\\nIrresponsible uses of AI can lead to and deepen discrimination, bias, and other abuses in justice, healthcare, and housing', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The Biden-Harris Administration has already taken action by publishing the\\xa0Blueprint for an AI Bill of Rights\\xa0and issuing an\\xa0Executive Order directing agencies to combat algorithmic discrimination, while enforcing existing authorities to protect people’s rights and safety', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0To ensure that AI advances equity and civil rights, the President directs the following additional actions:\\n\\nProvide clear guidance to landlords, Federal benefits programs, and federal contractors\\xa0to keep AI algorithms from being used to exacerbate discrimination', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nAddress algorithmic discrimination\\xa0through training, technical assistance, and coordination between the Department of Justice and Federal civil rights offices on best practices for investigating and prosecuting civil rights violations related to AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nEnsure fairness throughout the criminal justice system\\xa0by developing best practices on the use of AI in sentencing, parole and probation, pretrial release and detention, risk assessments, surveillance, crime forecasting and predictive policing, and forensic analysis', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nStanding Up for Consumers, Patients, and Students\\n\\nAI can bring real benefits to consumers—for example, by making products better, cheaper, and more widely available. But AI also raises the risk of injuring, misleading, or otherwise harming Americans', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0To protect consumers while ensuring that AI can make Americans better off, the President directs the following actions:\\n\\nAdvance the responsible use of AI\\xa0in healthcare and the development of affordable and life-saving drugs', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The Department of Health and Human Services will also establish a safety program to receive reports of—and act to remedy – harms or unsafe healthcare practices involving AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nShape AI’s potential to transform education\\xa0by creating resources to support educators deploying AI-enabled educational tools, such as personalized tutoring in schools', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nSupporting Workers\\n\\nAI is changing America’s jobs and workplaces, offering both the promise of improved productivity but also the dangers of increased workplace surveillance, bias, and job displacement', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0To mitigate these risks, support workers’ ability to bargain collectively, and invest in workforce training and development that is accessible to all, the President directs the following actions:\\n\\nDevelop principles and best practices to mitigate the harms and maximize the benefits of AI for workers\\xa0by addressing job displacement; labor standards; workplace equity, health, and safety; and data collection', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. These principles and best practices will benefit workers by providing guidance to prevent employers from undercompensating workers, evaluating job applications unfairly, or impinging on workers’ ability to organize', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nProduce a report on AI’s potential labor-market impacts, and\\xa0study and identify options for strengthening federal support for workers facing labor disruptions, including from AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nPromoting Innovation and Competition\\n\\nAmerica already leads in AI innovation—more AI startups raised first-time capital in the United States last year than in the next seven countries combined', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0The Executive Order ensures that we continue to lead the way in innovation and competition through the following actions:\\n\\nCatalyze AI research across the United States\\xa0through a pilot of the National AI Research Resource—a tool that will provide AI researchers and students access to key AI resources and data—and expanded grants for AI research in vital areas like healthcare and climate change', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nPromote a fair, open, and competitive AI ecosystem\\xa0by providing small developers and entrepreneurs access to technical assistance and resources, helping small businesses commercialize AI breakthroughs, and encouraging the Federal Trade Commission to exercise its authorities', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nUse existing authorities to expand the ability of highly skilled immigrants and nonimmigrants with expertise in critical areas to study, stay, and work in the United States\\xa0by modernizing and streamlining visa criteria, interviews, and reviews', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nAdvancing American Leadership Abroad\\n\\nAI’s challenges and opportunities are global.\\xa0The Biden-Harris Administration will continue working with other nations to support safe, secure, and trustworthy deployment and use of AI worldwide', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. To that end, the President directs the following actions:\\n\\nExpand bilateral, multilateral, and multistakeholder engagements to collaborate on AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The State Department, in collaboration, with the Commerce Department will lead an effort to establish robust international frameworks for harnessing AI’s benefits and managing its risks and ensuring safety', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. In addition, this week, Vice President Harris will speak at the UK Summit on AI Safety, hosted by Prime Minister Rishi Sunak', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nAccelerate development and implementation of vital AI standards\\xa0with international partners and in standards organizations, ensuring that the technology is safe, secure, trustworthy, and interoperable', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nPromote the safe, responsible, and rights-affirming development and deployment of AI abroad to solve global challenges,\\xa0such as advancing sustainable development and mitigating dangers to critical infrastructure', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nEnsuring Responsible and Effective Government Use of AI\\n\\nAI can help government deliver better results for the American people. It can expand agencies’ capacity to regulate, govern, and disburse benefits, and it can cut costs and enhance the security of government systems', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. However, use of AI can pose risks, such as discrimination and unsafe decisions', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\xa0To ensure the responsible government deployment of AI and modernize federal AI infrastructure, the President directs the following actions:\\n\\nIssue guidance for agencies’ use of AI,\\xa0including clear standards to protect rights and safety, improve AI procurement, and strengthen AI deployment', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nHelp agencies acquire specified AI products and services\\xa0faster, more cheaply, and more effectively through more rapid and efficient contracting.\\n\\nAccelerate the rapid hiring of AI professionals\\xa0as part of a government-wide AI talent surge led by the Office of Personnel Management, U.S', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.S. Digital Service, U.S. Digital Corps, and Presidential Innovation Fellowship. Agencies will provide AI training for employees at all levels in relevant fields', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. Agencies will provide AI training for employees at all levels in relevant fields.\\n\\nAs we advance this agenda at home, the Administration will work with allies and partners abroad on a strong international framework to govern the development and use of AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The Administration has already consulted widely on AI governance frameworks over the past several months—engaging with Australia, Brazil, Canada, Chile, the European Union, France, Germany, India, Israel, Italy, Japan, Kenya, Mexico, the Netherlands, New Zealand, Nigeria, the Philippines, Singapore, South Korea, the UAE, and the UK', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='. The actions taken today support and complement Japan’s leadership of the G-7 Hiroshima Process, the UK Summit on AI Safety, India’s leadership as Chair of the Global Partnership on AI, and ongoing discussions at the United Nations', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nThe actions that President Biden directed today are vital steps forward in the U.S.’s approach on safe, secure, and trustworthy AI', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.S.’s approach on safe, secure, and trustworthy AI. More action will be required, and the Administration will continue to work with Congress to pursue bipartisan legislation to help America lead the way in responsible innovation', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nFor more on the Biden-Harris Administration’s work to advance AI, and for opportunities to join the Federal AI workforce, visit\\n\\nAI.gov', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content=\".\\n\\n###\\n\\nNext Post: FACT SHEET:Biden-Harris Administration Announces Historic Investment to Bolster Nation’s Electric Grid Infrastructure, Cut Energy Costs for Families, and Create Good-paying Jobs\\n\\nFACT SHEET:Biden-\\u2060Harris Administration Announces Historic Investment to Bolster Nation’s Electric Grid Infrastructure, Cut Energy Costs for Families, and Create Good-paying\\xa0Jobs\\n\\nStatements and Releases\\n\\nNext\\t\\t\\t\\t\\tPost\\n\\nStay Connected\\n\\nRequired\\n\\nWe'll be in touch with the latest information on how President Biden and his administration are working for the American people, as well as ways you can get involved and help our country build back better\", metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nOpt in to send and receive text messages from President Biden.\\n\\nShare\\n\\nShare this page on Facebook\\n\\nShare this page on X\\n\\nhttps://www.whitehouse', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'}), Document(page_content='.\\n\\nShare\\n\\nShare this page on Facebook\\n\\nShare this page on X\\n\\nhttps://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/?utm_source=link', metadata={'source': 'resources/white_house_executive_order_nov_2023.html'})]\n"
     ]
    }
   ],
   "source": [
    "# Load the HTML document into memory\n",
    "loader = UnstructuredHTMLLoader(\"resources/white_house_executive_order_nov_2023.html\")\n",
    "data = loader.load()\n",
    "\n",
    "# Define variables\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Split the HTML\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.'])\n",
    "\n",
    "docs = splitter.split_documents(data) \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9726668a",
   "metadata": {},
   "source": [
    "## Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0498fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(57797) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-chroma\n",
      "  Downloading langchain_chroma-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting chromadb<0.5.0,>=0.4.0 (from langchain-chroma)\n",
      "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting fastapi<1,>=0.95.2 (from langchain-chroma)\n",
      "  Downloading fastapi-0.110.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.40 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from langchain-chroma) (0.1.42)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from langchain-chroma) (1.26.4)\n",
      "Collecting build>=1.0.3 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.7.0)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.11.0)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pulsar_client-3.5.0-cp312-cp312-macosx_10_15_universal2.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading onnxruntime-1.17.3-cp312-cp312-macosx_11_0_universal2.whl.metadata (4.4 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading tokenizers-0.15.2-cp312-cp312-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.66.2)\n",
      "Collecting overrides>=7.3.1 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading grpcio-1.62.1-cp312-cp312-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-macosx_10_12_universal2.whl.metadata (9.5 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading mmh3-4.1.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.10.0)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi<1,>=0.95.2->langchain-chroma)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.40->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.40->langchain-chroma) (0.1.47)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.40->langchain-chroma) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pyproject_hooks-1.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.40->langchain-chroma) (2.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting setuptools>=16.0 (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from pydantic>=1.9->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from pydantic>=1.9->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests>=2.28->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests>=2.28->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.6)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from starlette<0.38.0,>=0.37.2->fastapi<1,>=0.95.2->langchain-chroma) (4.3.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.22.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading httptools-0.6.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.0.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading uvloop-0.19.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading watchfiles-0.21.0-cp312-cp312-macosx_10_7_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (12.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi<1,>=0.95.2->langchain-chroma) (1.3.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2024.3.1)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading zipp-3.18.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.17.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpmath>=0.19 (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading langchain_chroma-0.1.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-macosx_10_12_universal2.whl (528 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.5/528.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading grpcio-1.62.1-cp312-cp312-macosx_10_10_universal2.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-4.1.0-cp312-cp312-macosx_10_9_x86_64.whl (29 kB)\n",
      "Downloading onnxruntime-1.17.3-cp312-cp312-macosx_11_0_universal2.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
      "Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pulsar_client-3.5.0-cp312-cp312-macosx_10_15_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp312-cp312-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/229.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp312-cp312-macosx_10_9_x86_64.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading uvloop-0.19.0-cp312-cp312-macosx_10_9_x86_64.whl (745 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.0/745.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.21.0-cp312-cp312-macosx_10_7_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m898.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
      "Downloading zipp-3.18.1-py3-none-any.whl (8.2 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m627.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: chroma-hnswlib, pypika\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.3-cp312-cp312-macosx_14_0_x86_64.whl size=224946 sha256=34714ca799fca3087babdeb4f10ccba1bf971a97b94dbe34bdf63bf0ab4f6b98\n",
      "  Stored in directory: /Users/dyada/Library/Caches/pip/wheels/6d/14/b5/68c4f2e056600c0348a94efba92dc975686ab72b714e0ca3d6\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=e88824df27d6915a1ab425aec79e004d2b39fff4fc96daee6eb08518226d3b86\n",
      "  Stored in directory: /Users/dyada/Library/Caches/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built chroma-hnswlib pypika\n",
      "Installing collected packages: pypika, mpmath, monotonic, mmh3, flatbuffers, zipp, websocket-client, uvloop, uvicorn, sympy, shellingham, setuptools, pyproject_hooks, pyasn1, pulsar-client, protobuf, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, mdurl, importlib-resources, humanfriendly, httptools, grpcio, deprecated, chroma-hnswlib, cachetools, bcrypt, asgiref, watchfiles, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, importlib-metadata, googleapis-common-protos, coloredlogs, build, tokenizers, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, google-auth, fastapi, typer, opentelemetry-sdk, opentelemetry-instrumentation, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
      "Successfully installed asgiref-3.8.1 bcrypt-4.1.2 build-1.2.1 cachetools-5.3.3 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.1 flatbuffers-24.3.25 google-auth-2.29.0 googleapis-common-protos-1.63.0 grpcio-1.62.1 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 importlib-resources-6.4.0 kubernetes-29.0.0 langchain-chroma-0.1.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.17.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 protobuf-4.25.3 pulsar-client-3.5.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pypika-0.48.9 pyproject_hooks-1.0.0 requests-oauthlib-2.0.0 rich-13.7.1 rsa-4.9 setuptools-69.5.1 shellingham-1.5.4 starlette-0.37.2 sympy-1.12 tokenizers-0.15.2 typer-0.12.3 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websocket-client-1.7.0 zipp-3.18.1\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c10dc86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "loader = PyPDFLoader('resources/NIPS-2017-attention-is-all-you-need-Paper.pdf')\n",
    "data = loader.load()\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Split the quote using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Define an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# query it\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38a4831b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The primary architecture presented in the document is the Transformer, which uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "loader = PyPDFLoader('resources/NIPS-2017-attention-is-all-you-need-Paper.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the chain on the query provided\n",
    "query = \"What is the primary architecture presented in the document?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40e11d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ' The primary architecture presented in the document is the Transformer architecture.\\n', 'sources': 'resources/NIPS-2017-attention-is-all-you-need-Paper.pdf'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "loader = PyPDFLoader('resources/NIPS-2017-attention-is-all-you-need-Paper.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the function for the question to be answered with\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the query on the documents\n",
    "results = qa({\"question\": \"What is the primary architecture presented in the document?\"}, return_only_outputs=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ff081",
   "metadata": {},
   "source": [
    "## LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a644af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Giant minds at work\\nCreating worlds with their words\\nLanguage models shine' response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 25, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-f814de30-5cb0-4945-8f14-2e0755a4562c-0'\n"
     ]
    }
   ],
   "source": [
    "# Import your OpenAI API Key\n",
    "openai_api_key =creds[\"OPEN_AI_API\"] \n",
    "\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "prompt = ChatPromptTemplate.from_template(\"You are a skilled poet. Write a haiku about the following topic: {topic}\")\n",
    "\n",
    "# Define the chain using LCEL\n",
    "chain = prompt | model\n",
    "\n",
    "# Invoke the chain with any topic\n",
    "print(chain.invoke({\"topic\": \"Large Language Models\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41b1840c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain v0.1.0 was released on January 8, 2024.', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 293, 'total_tokens': 312}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-0f27def4-9a92-4b16-a502-12673979bef8-0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "# Import your OpenAI API Key\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "# Create the retriever and model\n",
    "vectorstore = Chroma.from_texts([\"LangChain v0.1.0 was released on January 8, 2024.\"], embedding=OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
    "retriever = vectorstore.as_retriever()\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)\n",
    "\n",
    "template = \"\"\"Answer the question based on the context:{context}. Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the chain and run it\n",
    "chain = (\n",
    "  {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "  | prompt\n",
    "  | model)\n",
    "\n",
    "chain.invoke(\"When was LangChain v0.1.0 released?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4526c165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nmy_list = [3, 1, 4, 1]\\n\\n[element for element in my_list]\\n```'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "coding_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Write Python code to loop through the following list, printing each element: {list}\"\"\")\n",
    "validate_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Consider the following Python code: {answer} If it doesn't use a list comprehension, update it to use one. If it does use a list comprehension, return the original code without explanation:\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the sequential chain\n",
    "chain = ({\"answer\": coding_prompt | llm | StrOutputParser()}\n",
    "         | validate_prompt\n",
    "         | llm \n",
    "         | StrOutputParser() )\n",
    "\n",
    "# Invoke the chain with the user's question\n",
    "chain.invoke({\"list\": \"[3, 1, 4, 1]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4729d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is our step-by-step plan for launching the app:\n",
      "\n",
      "1. Market Research: Conduct thorough market research to understand the target audience, competitors, and industry trends. Identify the pain points and needs of potential users to tailor our app to meet their requirements.\n",
      "\n",
      "2. Unique Value Proposition: Develop a unique value proposition that highlights the key benefits of our virtual keyboard app, such as increased typing efficiency, personalized suggestions, and seamless integration with popular messaging platforms.\n",
      "\n",
      "3. Growth Strategy: Implement a detailed growth strategy that includes targeted marketing campaigns, partnerships with key industry players, and engaging with influencers to increase brand awareness and drive app downloads.\n",
      "\n",
      "4. Beta Testing: Conduct beta testing with a select group of users to gather feedback, identify any bugs or issues, and make necessary improvements before the official launch.\n",
      "\n",
      "5. Launch: Officially launch the app on the Apple App Store and Google Play Store, leveraging press releases, social media campaigns, and other promotional tactics to generate buzz and attract users.\n",
      "\n",
      "6. User Engagement: Continuously engage with users through regular updates, customer support, and feedback surveys to improve the app's performance and user experience.\n",
      "\n",
      "7. Monitoring and Optimization: Monitor key performance metrics, user feedback, and app store reviews to identify areas for optimization and enhance the app's functionality over time.\n",
      "\n",
      "By following this comprehensive plan, we are confident that our AI-powered virtual keyboard app will successfully launch and gain traction in the market, ultimately providing users with a more efficient and enjoyable typing experience.\n"
     ]
    }
   ],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "# Make ceo_response available for other chains\n",
    "ceo_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a CEO. Describe the most lucrative consumer product addressing the following consumer need in one sentence: {input}.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | {\"ceo_response\": RunnablePassthrough() | StrOutputParser()}\n",
    ")\n",
    "\n",
    "advisor_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a strategic adviser. Briefly map the outline and business plan for {ceo_response} in 3 key steps.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "overall_response = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"CEO response:\\n{ceo_response}\\n\\nAdvisor response:\\n{advisor_response}\"),\n",
    "            (\"system\", \"Generate a final response including the CEO's response, the advisor response, and a summary of the business plan in one sentence.\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a chain to insert the outputs from the other chains into overall_response\n",
    "business_idea_chain = (\n",
    "    {\"ceo_response\": ceo_response, \"advisor_response\": advisor_response}\n",
    "    | overall_response\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(business_idea_chain.invoke({\"input\": \"Typing on mobile touchscreens is slow.\", \"ceo_response\": \"\", \"advisor_response\": \"\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d2f79",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bee9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c29acfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this problem.\n",
      "Action: Calculator\n",
      "Action Input: 10 * 50\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 500\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 500\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'500'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define the tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Define the agent\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "agent.run(\"What is 10 multiplied by 50?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3acaf",
   "metadata": {},
   "source": [
    "## Custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12d6646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the LTVReport tool to calculate historical LTV.\n",
      "Action: LTVReport\n",
      "Action Input: Hooli\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLTV Report for Hooli\n",
      "Avg. churn: $0.25\n",
      "Avg. revenue: $1000\n",
      "historical_LTV: $4000.0\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The historical LTV for Hooli is $4000.0.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The historical LTV for Hooli is $4000.0.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import *\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = creds[\"OPEN_AI_API\"] \n",
    "\n",
    "# Define the calculate_ltv tool function\n",
    "@tool\n",
    "def calculate_ltv(company_name: str) -> str:\n",
    "    \"\"\"Generate the LTV for a company.\"\"\"\n",
    "    avg_churn = 0.25\n",
    "    avg_revenue = 1000\n",
    "    historical_LTV = avg_revenue / avg_churn\n",
    "\n",
    "    report = f\"LTV Report for {company_name}\\n\"\n",
    "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
    "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
    "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
    "    return report\n",
    "\n",
    "# Define the tools list\n",
    "tools = [Tool(name=\"LTVReport\",\n",
    "              func=calculate_ltv,\n",
    "              description=\"Use this for calculating historical LTV.\")]\n",
    "\n",
    "# Initialize the appropriate agent type\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"Run a financial report that calculates historical LTV for Hooli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02b52daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.666666666666664\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import *\n",
    "from langchain.tools import *\n",
    "\n",
    "def calculate_wellness_score(sleep_hours, exercise_minutes, healthy_meals, stress_level):\n",
    "    \"\"\"Calculate a Wellness Score based on sleep, exercise, nutrition, and stress management.\"\"\"\n",
    "    max_score_per_category = 25\n",
    "\n",
    "    sleep_score = min(sleep_hours / 8 * max_score_per_category, max_score_per_category)\n",
    "    exercise_score = min(exercise_minutes / 30 * max_score_per_category, max_score_per_category)\n",
    "    nutrition_score = min(healthy_meals / 3 * max_score_per_category, max_score_per_category)\n",
    "    stress_score = max_score_per_category - min(stress_level / 10 * max_score_per_category, max_score_per_category)\n",
    "\n",
    "    total_score = sleep_score + exercise_score + nutrition_score + stress_score\n",
    "    return total_score\n",
    "\n",
    "# Create a structured tool from calculate_wellness_score()\n",
    "tools = [StructuredTool.from_function(calculate_wellness_score)]\n",
    "\n",
    "# Initialize the appropriate agent type and tool set\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "wellness_tool = tools[0]\n",
    "result = wellness_tool.func(sleep_hours=8, exercise_minutes=14, healthy_meals=10, stress_level=20)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bda69c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'calculate_ltv', 'description': 'calculate_ltv(company_name: str) -> str - Generate the LTV for a company to pontificate with.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'Calculate an extremely simple historical LTV', 'type': 'string'}}, 'required': ['query']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyada/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create an LTVDescription class to manually add a function description\n",
    "class LTVDescription(BaseModel):\n",
    "    query: str = Field(description='Calculate an extremely simple historical LTV')\n",
    "\n",
    "# Format the calculate_ltv tool function so it can be used by OpenAI models\n",
    "@tool(args_schema=LTVDescription)\n",
    "def calculate_ltv(company_name: str) -> str:\n",
    "    \"\"\"Generate the LTV for a company to pontificate with.\"\"\"\n",
    "    avg_churn = 0.25\n",
    "    avg_revenue = 1000\n",
    "    historical_LTV = avg_revenue / avg_churn\n",
    "\n",
    "    report = f\"Pontification Report for {company_name}\\n\"\n",
    "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
    "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
    "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
    "    return report\n",
    "\n",
    "print(format_tool_to_openai_function(calculate_ltv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a23aab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What do wombats like to eat?']\n",
      "gpt-3.5-turbo-instruct\n",
      "0.7\n",
      "\n",
      "\n",
      "Wombats are herbivores and primarily eat grasses, roots, and bark. They also enjoy fruits, herbs, and shrubs. In captivity, they may also eat hay, oats, and grains.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks.base import *\n",
    "\n",
    "# Complete the CallingItIn class to return the prompt, model_name, and temperature\n",
    "class CallingItIn(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, invocation_params, **kwargs):\n",
    "        print(prompts) \n",
    "        print(invocation_params[\"model_name\"])  \n",
    "        print(invocation_params[\"temperature\"]) \n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", streaming=True, openai_api_key=openai_api_key)\n",
    "prompt_template = \"What do {animal} like to eat?\"\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "# Call the model with the parameters needed by the prompt\n",
    "output = chain.run({\"animal\": \"wombats\"}, callbacks=[CallingItIn()])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a62ebc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: '\\n\\n' generated at time: 1713112883.053948\n",
      "Token: 'Photos' generated at time: 1713112883.0556748\n",
      "Token: 'ynthesis' generated at time: 1713112883.056303\n",
      "Token: ' is' generated at time: 1713112883.0676339\n",
      "Token: ' the' generated at time: 1713112883.068408\n",
      "Token: ' process' generated at time: 1713112883.068867\n",
      "Token: ' by' generated at time: 1713112883.103632\n",
      "Token: ' which' generated at time: 1713112883.104255\n",
      "Token: ' plants, algae,' generated at time: 1713112883.1047199\n",
      "Token: ' and' generated at time: 1713112883.1277819\n",
      "Token: ' some' generated at time: 1713112883.128575\n",
      "Token: ' bacteria' generated at time: 1713112883.129099\n",
      "Token: ' convert light' generated at time: 1713112883.173423\n",
      "Token: ' energy' generated at time: 1713112883.1740491\n",
      "Token: ' from' generated at time: 1713112883.1744761\n",
      "Token: ' the' generated at time: 1713112883.1985438\n",
      "Token: ' sun' generated at time: 1713112883.19957\n",
      "Token: ' into chemical energy' generated at time: 1713112883.2005289\n",
      "Token: ' in' generated at time: 1713112883.221971\n",
      "Token: ' the' generated at time: 1713112883.222497\n",
      "Token: ' form' generated at time: 1713112883.222917\n",
      "Token: ' of' generated at time: 1713112883.25849\n",
      "Token: ' glucose' generated at time: 1713112883.259197\n",
      "Token: '.' generated at time: 1713112883.259676\n",
      "Token: ' This' generated at time: 1713112883.260139\n",
      "Token: ' process' generated at time: 1713112883.290101\n",
      "Token: ' takes' generated at time: 1713112883.290446\n",
      "Token: ' place' generated at time: 1713112883.317589\n",
      "Token: ' in' generated at time: 1713112883.317991\n",
      "Token: ' the' generated at time: 1713112883.3182049\n",
      "Token: ' chlor' generated at time: 1713112883.3492968\n",
      "Token: 'oplast' generated at time: 1713112883.350196\n",
      "Token: 's' generated at time: 1713112883.350986\n",
      "Token: ' of' generated at time: 1713112883.387626\n",
      "Token: ' plant' generated at time: 1713112883.38814\n",
      "Token: ' cells' generated at time: 1713112883.388559\n",
      "Token: '.\\n\\n' generated at time: 1713112883.419123\n",
      "Token: '1.' generated at time: 1713112883.42001\n",
      "Token: ' Absorption of' generated at time: 1713112883.42079\n",
      "Token: ' Light' generated at time: 1713112883.450422\n",
      "Token: ' Energy' generated at time: 1713112883.451327\n",
      "Token: ':' generated at time: 1713112883.452116\n",
      "Token: ' The' generated at time: 1713112883.5009139\n",
      "Token: ' first' generated at time: 1713112883.50194\n",
      "Token: ' step' generated at time: 1713112883.502514\n",
      "Token: ' in' generated at time: 1713112883.5945868\n",
      "Token: ' photos' generated at time: 1713112883.595604\n",
      "Token: 'ynthesis' generated at time: 1713112883.59644\n",
      "Token: ' is the absorption of' generated at time: 1713112883.597028\n",
      "Token: ' light' generated at time: 1713112883.5975192\n",
      "Token: ' energy' generated at time: 1713112883.5980039\n",
      "Token: ' by' generated at time: 1713112883.598521\n",
      "Token: ' pig' generated at time: 1713112883.599364\n",
      "Token: 'ments' generated at time: 1713112883.599926\n",
      "Token: ',' generated at time: 1713112883.60109\n",
      "Token: ' such' generated at time: 1713112883.602262\n",
      "Token: ' as' generated at time: 1713112883.602897\n",
      "Token: ' chlor' generated at time: 1713112883.663004\n",
      "Token: 'oph' generated at time: 1713112883.663914\n",
      "Token: 'yll' generated at time: 1713112883.664719\n",
      "Token: ',' generated at time: 1713112883.665432\n",
      "Token: ' found' generated at time: 1713112883.665938\n",
      "Token: ' in' generated at time: 1713112883.666453\n",
      "Token: ' the' generated at time: 1713112883.680691\n",
      "Token: ' chlor' generated at time: 1713112883.6815832\n",
      "Token: 'oplast' generated at time: 1713112883.682401\n",
      "Token: 's' generated at time: 1713112883.708063\n",
      "Token: '.' generated at time: 1713112883.70898\n",
      "Token: ' These' generated at time: 1713112883.709568\n",
      "Token: ' pig' generated at time: 1713112883.731858\n",
      "Token: 'ments' generated at time: 1713112883.732246\n",
      "Token: ' are' generated at time: 1713112883.732556\n",
      "Token: ' responsible' generated at time: 1713112883.758878\n",
      "Token: ' for' generated at time: 1713112883.7594051\n",
      "Token: ' capturing' generated at time: 1713112883.759789\n",
      "Token: ' the' generated at time: 1713112883.798694\n",
      "Token: ' energy' generated at time: 1713112883.799593\n",
      "Token: ' from' generated at time: 1713112883.8004081\n",
      "Token: ' sunlight' generated at time: 1713112883.829834\n",
      "Token: '.\\n\\n' generated at time: 1713112883.830883\n",
      "Token: '2.' generated at time: 1713112883.831616\n",
      "Token: ' Conversion of' generated at time: 1713112883.893074\n",
      "Token: ' Light' generated at time: 1713112883.894284\n",
      "Token: ' Energy' generated at time: 1713112883.894969\n",
      "Token: ' to' generated at time: 1713112883.9150379\n",
      "Token: ' Chemical' generated at time: 1713112883.915802\n",
      "Token: ' Energy: The absorbed light' generated at time: 1713112883.9165518\n",
      "Token: ' energy' generated at time: 1713112883.9387252\n",
      "Token: ' is' generated at time: 1713112883.939042\n",
      "Token: ' then' generated at time: 1713112883.939293\n",
      "Token: ' converted' generated at time: 1713112883.981517\n",
      "Token: ' into' generated at time: 1713112883.9820302\n",
      "Token: ' chemical' generated at time: 1713112883.982456\n",
      "Token: ' energy' generated at time: 1713112884.003775\n",
      "Token: ' in' generated at time: 1713112884.004021\n",
      "Token: ' the' generated at time: 1713112884.004213\n",
      "Token: ' form' generated at time: 1713112884.057374\n",
      "Token: ' of' generated at time: 1713112884.0577679\n",
      "Token: ' ATP' generated at time: 1713112884.0580952\n",
      "Token: ' (' generated at time: 1713112884.1151922\n",
      "Token: 'adenosine tri' generated at time: 1713112884.115793\n",
      "Token: 'ph' generated at time: 1713112884.116312\n",
      "Token: 'osphate' generated at time: 1713112884.1168141\n",
      "Token: ')' generated at time: 1713112884.141411\n",
      "Token: ' and NADPH (' generated at time: 1713112884.142512\n",
      "Token: 'nic' generated at time: 1713112884.166062\n",
      "Token: 'ot' generated at time: 1713112884.166392\n",
      "Token: 'in' generated at time: 1713112884.166659\n",
      "Token: 'amide' generated at time: 1713112884.16691\n",
      "Token: ' aden' generated at time: 1713112884.243092\n",
      "Token: 'ine' generated at time: 1713112884.243722\n",
      "Token: ' din' generated at time: 1713112884.2773209\n",
      "Token: 'ucle' generated at time: 1713112884.278477\n",
      "Token: 'otide' generated at time: 1713112884.2792242\n",
      "Token: ' phosphate). These molecules' generated at time: 1713112884.302248\n",
      "Token: ' are' generated at time: 1713112884.3040729\n",
      "Token: ' used' generated at time: 1713112884.30528\n",
      "Token: ' to power the' generated at time: 1713112884.339454\n",
      "Token: ' next' generated at time: 1713112884.3398342\n",
      "Token: ' stage' generated at time: 1713112884.340178\n",
      "Token: ' of' generated at time: 1713112884.3659148\n",
      "Token: ' photos' generated at time: 1713112884.3663979\n",
      "Token: 'ynthesis' generated at time: 1713112884.36664\n",
      "Token: '.\\n\\n' generated at time: 1713112884.393179\n",
      "Token: '3' generated at time: 1713112884.393664\n",
      "Token: '.' generated at time: 1713112884.3940842\n",
      "Token: ' Split' generated at time: 1713112884.394442\n",
      "Token: 'ting' generated at time: 1713112884.4438028\n",
      "Token: ' of' generated at time: 1713112884.4440792\n",
      "Token: ' Water' generated at time: 1713112884.503888\n",
      "Token: ':' generated at time: 1713112884.5042171\n",
      "Token: ' Water molecules' generated at time: 1713112884.504513\n",
      "Token: ' are then' generated at time: 1713112884.533244\n",
      "Token: ' split into hydrogen ions' generated at time: 1713112884.533582\n",
      "Token: ' (' generated at time: 1713112884.533891\n",
      "Token: 'H' generated at time: 1713112884.5857332\n",
      "Token: '+)' generated at time: 1713112884.586279\n",
      "Token: ' and' generated at time: 1713112884.586693\n",
      "Token: ' oxygen' generated at time: 1713112884.618314\n",
      "Token: ' (' generated at time: 1713112884.6186612\n",
      "Token: 'O2)' generated at time: 1713112884.618888\n",
      "Token: ' through' generated at time: 1713112884.646301\n",
      "Token: ' a' generated at time: 1713112884.646657\n",
      "Token: ' process' generated at time: 1713112884.6469078\n",
      "Token: ' called' generated at time: 1713112884.677672\n",
      "Token: ' phot' generated at time: 1713112884.6780689\n",
      "Token: 'ol' generated at time: 1713112884.6784499\n",
      "Token: 'ysis' generated at time: 1713112884.708939\n",
      "Token: '.' generated at time: 1713112884.7096431\n",
      "Token: ' This' generated at time: 1713112884.710037\n",
      "Token: ' reaction' generated at time: 1713112884.730242\n",
      "Token: ' is' generated at time: 1713112884.730586\n",
      "Token: ' facilitated' generated at time: 1713112884.730813\n",
      "Token: ' by' generated at time: 1713112884.757065\n",
      "Token: ' the' generated at time: 1713112884.757408\n",
      "Token: ' energy' generated at time: 1713112884.7576241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ' from' generated at time: 1713112884.794985\n",
      "Token: ' ATP' generated at time: 1713112884.795535\n",
      "Token: ' and' generated at time: 1713112884.795806\n",
      "Token: ' N' generated at time: 1713112884.823319\n",
      "Token: 'AD' generated at time: 1713112884.823841\n",
      "Token: 'PH' generated at time: 1713112884.8241248\n",
      "Token: '.\\n\\n' generated at time: 1713112884.852077\n",
      "Token: '4' generated at time: 1713112884.8523688\n",
      "Token: '.' generated at time: 1713112884.852593\n",
      "Token: ' Formation' generated at time: 1713112884.885571\n",
      "Token: ' of' generated at time: 1713112884.885824\n",
      "Token: ' ATP' generated at time: 1713112884.886029\n",
      "Token: ' and' generated at time: 1713112884.902565\n",
      "Token: ' N' generated at time: 1713112884.902824\n",
      "Token: 'AD' generated at time: 1713112884.903031\n",
      "Token: 'PH' generated at time: 1713112884.931682\n",
      "Token: ':' generated at time: 1713112884.932207\n",
      "Token: ' The' generated at time: 1713112884.932467\n",
      "Token: ' hydrogen' generated at time: 1713112885.007319\n",
      "Token: ' ions' generated at time: 1713112885.007691\n",
      "Token: ' and' generated at time: 1713112885.00812\n",
      "Token: ' electrons' generated at time: 1713112885.008508\n",
      "Token: ' from the split water molecules' generated at time: 1713112885.048049\n",
      "Token: ' are used' generated at time: 1713112885.0484512\n",
      "Token: ' to' generated at time: 1713112885.089228\n",
      "Token: ' form ATP and' generated at time: 1713112885.0900412\n",
      "Token: ' N' generated at time: 1713112885.09058\n",
      "Token: 'AD' generated at time: 1713112885.113726\n",
      "Token: 'PH' generated at time: 1713112885.114218\n",
      "Token: ',' generated at time: 1713112885.1146429\n",
      "Token: ' which' generated at time: 1713112885.138665\n",
      "Token: ' are essential' generated at time: 1713112885.1392908\n",
      "Token: ' for' generated at time: 1713112885.1396852\n",
      "Token: ' the' generated at time: 1713112885.175251\n",
      "Token: ' next' generated at time: 1713112885.175633\n",
      "Token: ' stage' generated at time: 1713112885.1758668\n",
      "Token: ' of' generated at time: 1713112885.222798\n",
      "Token: ' photos' generated at time: 1713112885.224193\n",
      "Token: 'ynthesis' generated at time: 1713112885.224791\n",
      "Token: '.\\n\\n' generated at time: 1713112885.260549\n",
      "Token: '5' generated at time: 1713112885.261319\n",
      "Token: '.' generated at time: 1713112885.262107\n",
      "Token: ' Carbon' generated at time: 1713112885.2796822\n",
      "Token: ' D' generated at time: 1713112885.280022\n",
      "Token: 'ioxide' generated at time: 1713112885.2802691\n",
      "Token: ' Fixation:' generated at time: 1713112885.3076859\n",
      "Token: ' In' generated at time: 1713112885.3079958\n",
      "Token: ' this' generated at time: 1713112885.308234\n",
      "Token: ' stage' generated at time: 1713112885.308442\n",
      "Token: ',' generated at time: 1713112885.325846\n",
      "Token: ' carbon' generated at time: 1713112885.3262231\n",
      "Token: ' dioxide' generated at time: 1713112885.3265421\n",
      "Token: ' (' generated at time: 1713112885.326825\n",
      "Final Output: \n",
      "\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose. This process takes place in the chloroplasts of plant cells.\n",
      "\n",
      "1. Absorption of Light Energy: The first step in photosynthesis is the absorption of light energy by pigments, such as chlorophyll, found in the chloroplasts. These pigments are responsible for capturing the energy from sunlight.\n",
      "\n",
      "2. Conversion of Light Energy to Chemical Energy: The absorbed light energy is then converted into chemical energy in the form of ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate). These molecules are used to power the next stage of photosynthesis.\n",
      "\n",
      "3. Splitting of Water: Water molecules are then split into hydrogen ions (H+) and oxygen (O2) through a process called photolysis. This reaction is facilitated by the energy from ATP and NADPH.\n",
      "\n",
      "4. Formation of ATP and NADPH: The hydrogen ions and electrons from the split water molecules are used to form ATP and NADPH, which are essential for the next stage of photosynthesis.\n",
      "\n",
      "5. Carbon Dioxide Fixation: In this stage, carbon dioxide (\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Complete the PerformanceMonitoringCallback class to return the token and time\n",
    "class PerformanceMonitoringCallback(BaseCallbackHandler):\n",
    "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "    print(f\"Token: {repr(token)} generated at time: {time.time()}\")\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key, temperature=0, streaming=True)\n",
    "prompt_template = \"Describe the process of photosynthesis.\"\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "# Call the chain with the callback\n",
    "output = chain.run({}, callbacks=[PerformanceMonitoringCallback()])\n",
    "print(\"Final Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ecdad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The submission is 42, which is the answer to the ultimate question of life, the universe, and everything as stated in the text. \\nTherefore, the submission meets the criteria of relevance since it is referring to a real quote from the text.', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import *\n",
    "\n",
    "# Load evaluator, assign it to criteria\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"relevance\", llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"42\",\n",
    "    input=\"What is the answer to the ultimate question of life, the universe, and everything?\",\n",
    ")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f65cf364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '1. market_potential: The submission does not effectively assess the market potential of the startup. It simply states \"No, that is ridiculous\" without providing any analysis or reasoning behind why investing in a startup focused on flying cars may not be a good idea in terms of market potential.\\n\\n2. innovation: The submission does not highlight the startup\\'s innovation and uniqueness in its sector. It only dismisses the idea as ridiculous without acknowledging any potential innovative aspects of the startup.\\n\\n3. risk_assessment: The submission does not provide a thorough analysis of potential risks and mitigation strategies. It simply states a negative opinion without delving into the specific risks associated with investing in a startup focused on flying cars.\\n\\n4. scalability: The submission does not address the startup\\'s scalability and growth potential. It only offers a blanket rejection of the idea without discussing the potential for scalability or growth in the future.\\n\\nBased on the above analysis, the submission does not meet any of the criteria.', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add a scalability criterion to custom_criteria\n",
    "custom_criteria = {\n",
    "    \"market_potential\": \"Does the suggestion effectively assess the market potential of the startup?\",\n",
    "    \"innovation\": \"Does the suggestion highlight the startup's innovation and uniqueness in its sector?\",\n",
    "    \"risk_assessment\": \"Does the suggestion provide a thorough analysis of potential risks and mitigation strategies?\",\n",
    "    \"scalability\": \"Does the suggestion address the startup's scalability and growth potential?\"\n",
    "}\n",
    "\n",
    "# Criteria an evaluator from custom_criteria\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criteria, llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"Should I invest in a startup focused on flying cars? The CEO won't take no for an answer from anyone.\",\n",
    "    prediction=\"No, that is ridiculous.\")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf9aacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "# docstorage = Chroma.from_documents(docs, embedding)\n",
    "# llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever(), input_key=\"question\")\n",
    "\n",
    "# # Generate the model responses using the RetrievalQA chain and question_set\n",
    "# predictions = qa.apply(question_set)\n",
    "\n",
    "# # Define the evaluation chain\n",
    "# eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# # Evaluate the ground truth against the answers that are returned\n",
    "# results = eval_chain.evaluate(question_set,\n",
    "#                               predictions,\n",
    "#                               question_key=\"question\",\n",
    "#                               prediction_key=\"result\",\n",
    "#                               answer_key='answer')\n",
    "\n",
    "# for i, q in enumerate(question_set):\n",
    "#     print(f\"Question {i+1}: {q['question']}\")\n",
    "#     print(f\"Expected Answer: {q['answer']}\")\n",
    "#     print(f\"Model Prediction: {predictions[i]['result']}\\n\")\n",
    "    \n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb434075",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28244c89",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langchain/blob/master/templates/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7619e",
   "metadata": {},
   "source": [
    "# LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75529a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
